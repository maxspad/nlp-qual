{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: NLP-QuAL Results\n",
        "format:\n",
        "  html:\n",
        "    toc: true\n",
        "    echo: false\n",
        "---"
      ],
      "id": "3e7d0433"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "### Imports\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import display, Markdown\n",
        "import mlflow\n",
        "import os\n",
        "os.chdir('../../')\n",
        "print(os.getcwd())\n",
        "\n",
        "def md(s : str):\n",
        "    display(Markdown(s))\n",
        "\n",
        "def count_and_percent(df: pd.DataFrame, col : str):\n",
        "    qc = pd.DataFrame(df[col].value_counts()).sort_index().rename({col:'Count'}, axis=1)\n",
        "    qcn = (qc / len(df) * 100).rename({'Count':'Percent of Total'}, axis=1)\n",
        "    return pd.concat([qc, qcn], axis=1)\n",
        "\n",
        "### Load Data\n",
        "df = pd.read_pickle('data/interim/masterdbForNLPSpacyProc.pkl')\n",
        "train = pd.read_pickle('data/processed/train.pkl')\n",
        "test = pd.read_pickle('data/processed/test.pkl')"
      ],
      "id": "5148974d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Characteristics/Demographics"
      ],
      "id": "931eb3b2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "md(f\"\"\"\n",
        "We analyzed {len(df)} evaluations, with {len(df[df['dataSource'] == 'Mac'])} from Site 1 (McMaster) and {len(df[df['dataSource'] == 'Sas'])} from Site 2 (Saskatchewan). \n",
        "\"\"\")"
      ],
      "id": "3d04dd14",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For each evaluation, the QuAL score was rated by two separate raters. Each sub-score (Q1, Q2, and Q3) was rated and then summed to get the final QuAL score. Discrepancies were broken by members of the study team (who?) \n"
      ],
      "id": "094243ab"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "md(f\"\"\"The average QuAL score was {df['QUAL'].mean()}, with standard deviation \n",
        "{df['QUAL'].std():.3f}, median {df['QUAL'].median()}\"\"\")"
      ],
      "id": "c1923553",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Descriptive statistics for the subscores and QuAL Score were:"
      ],
      "id": "20be1fa3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "cols = ['Q1','Q2','Q3','QUAL']\n",
        "df[cols].describe().loc[['count','mean','std','50%'],:]"
      ],
      "id": "299952d7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Score Distributions\n",
        "\n",
        "#### Subscores\n",
        "The table below shows the count and associated percent for each level of each subscore."
      ],
      "id": "c8620a16"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pd.concat({\n",
        "    'Q1':count_and_percent(df, 'Q1'),\n",
        "    'Q2':count_and_percent(df, 'Q2'),\n",
        "    'Q3':count_and_percent(df, 'Q3')\n",
        "}, axis=1)"
      ],
      "id": "199a966b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The distribution for the subscores is plotted below"
      ],
      "id": "466c29d5"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| layout-ncol: 3\n",
        "#| fig-cap: Distribution of QuAL Subscores\n",
        "#| fig-subcap: \n",
        "#|  - \"Q1\"\n",
        "#|  - \"Q2\"\n",
        "#|  - \"Q3\"\n",
        "f = sns.countplot(x='Q1', hue='dataSource', data=df)\n",
        "plt.show(f)\n",
        "f2 = sns.countplot(x='Q2', hue='dataSource', data=df)\n",
        "plt.show(f2)\n",
        "f3 = sns.countplot(x='Q3', hue='dataSource', data=df)\n",
        "plt.show(f3)"
      ],
      "id": "5f93cd48",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluations tended to score highly (> 2) on Q1, but poorly (80% 0 and 83% 0, respectively) on Q2 and Q3. There were large differences in scores between the two sites.\n",
        "\n",
        "#### QuAL Score\n",
        "The table below shows the raw counts and percentages associated with each level of the rated QuAL score."
      ],
      "id": "ddde1434"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "count_and_percent(df, 'QUAL')"
      ],
      "id": "3cfe3148",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The distribution of the QuAL score is plotted below:"
      ],
      "id": "d7d3af06"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.show(sns.countplot(x='QUAL', hue='dataSource', data=df))"
      ],
      "id": "b02b46b9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This table shows the counts/frequencies for each possible combination of subscores. For each possible final QuAL score, the table shows the combination fo subscores most likely to generate that QuAL score."
      ],
      "id": "53b198f7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "vc = df[cols].value_counts(normalize=False)\n",
        "vc = pd.DataFrame(vc).reset_index()\n",
        "vc = vc.set_index(['QUAL','Q1','Q2','Q3'])\n",
        "vc = vc.sort_index().rename({0:'Count'}, axis=1)\n",
        "vc = pd.concat([vc, (vc / 2500 * 100).rename({'Count':'Percent of Total'}, axis=1)], axis=1)\n",
        "vc"
      ],
      "id": "124aea04",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This table yields some interesting insights. The QuAL score subscores are dependent on one another. Based on the structure of the subscores, it follows that if the evaluation is not detailed enough (Q1 $\\leq$ 2), it's unlikely to contain a suggestion for improvement (Q1 = 0), and there can be no linking between behavior and improvement (Q3 = 0). The table backs this up, with the vast majority of Q2 and Q3 rated as zero if Q1 $\\leq$ 2. If the evaluation is highly detailed (Q3 = 3), then naturally it is more likely to have a suggestion for improvement (Q2 = 1), and based on the table, it's also likely to link the suggestion to the behavior (Q3 = 1). Q3 is essentially redundant; Q3 is discrepant from Q3 in only 3.1% of all evaluations. \n",
        "\n",
        "This means that the QuAL score can be reduced to three primary outcomes:\n",
        "\n",
        "* Q1 $\\leq$ 2 - low detail, very unlikely (<10%) to contain suggestion for improvement \n",
        "* Q1 = 3; Q2 and Q3 = 0 - high detail,  no suggestion for improvement\n",
        "* Q1 = 3; Q2 and Q3 = 1 - high detail, with suggestion for improvement, extremely likely to have connection between behavior/suggestion\n",
        "\n",
        "These three scenarios fit 2,349/2,500 = 94% of evaluations. This provides an opportunity to condense the QuAL score from 6 levels (0-5) to 3. Although the remainder of the results below do *not* condense the QuAL score, this could be a good way to boost accuracy results in a way that does not compromise the integrity of the score itself. \n",
        "\n",
        "### Interrater Reliability\n",
        "This table shows the percent agreement for the QUAL score and each subscore"
      ],
      "id": "bddd8726"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pd.concat({\n",
        "    'Q1Match' : count_and_percent(df, 'Q1Match'),\n",
        "    'Q2Match' : count_and_percent(df, 'Q2Match'),\n",
        "    'Q3Match' : count_and_percent(df, 'Q3Match'),\n",
        "    'QUALMatch' : count_and_percent(df, 'perfectMatch')\n",
        "}, axis=1)"
      ],
      "id": "a1b8ce76",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.metrics import cohen_kappa_score\n",
        "pd.DataFrame(pd.Series([\n",
        "    cohen_kappa_score(df['q1p1'], df['q1p2']),\n",
        "    cohen_kappa_score(df['q2p1'], df['q2p2']),\n",
        "    cohen_kappa_score(df['q3p1'], df['q3p2']),\n",
        "    cohen_kappa_score(df['P1QualScore'], df['P2QualScore'])\n",
        "], index=['Q1','Q2','Q3','QuAL']), columns=['Cohen\\'s Kappa'])"
      ],
      "id": "a14e9677",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cohen's Kappas were calculated and are presented above. There was fair agreement for all scores except for Q1, which had substantial agreement. This was before any tiebreaking/discrepancy correction. \n",
        "\n",
        "### Other Demographics and Descriptive Statistics\n",
        "These may or may not be relevant. \n"
      ],
      "id": "2fbea887"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pd.concat({\n",
        "    c : count_and_percent(df, c)\n",
        "    for c in ['GenderRes','GenderFac']\n",
        "}, axis=1)"
      ],
      "id": "ff5e69b2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ObserverType stratifies the evaluator by role."
      ],
      "id": "57803b1c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pd.concat({\n",
        "    c : count_and_percent(df, c)\n",
        "    for c in ['ObserverType']\n",
        "}, axis=1)"
      ],
      "id": "4023572d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Only one of the two sites reports PGY levels for their trainees on their evaluations. "
      ],
      "id": "0aa4a255"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pd.DataFrame(df['PGY'].value_counts())"
      ],
      "id": "47cc0c46",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Performance\n",
        "\n",
        "### Q1"
      ],
      "id": "0e250daa"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "RUN_ID = '9feba93902bc44e0a08388f49fad81cf'\n",
        "run = mlflow.get_run(RUN_ID)\n",
        "print(run)"
      ],
      "id": "f4cc84a6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Q2\n",
        "\n",
        "### Q3\n",
        "\n",
        "### QuAL"
      ],
      "id": "42e12f7b"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}